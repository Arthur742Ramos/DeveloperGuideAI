# AI Effectiveness Evaluation Template

## Purpose

Evaluate whether AI is actually helping or just creating illusion of productivity. From Chapter 15.

## Quick Evaluation Prompts

### Post-Session Check

After any AI-assisted task, ask yourself:

```
Task: [what you did]
Time with AI: [how long]
Time fixing/verifying: [how long]
Estimated time without AI: [guess]

Was this faster? Better? What would I do differently?
```

### Break-Even Analysis

```
For this task:

Time to prompt and iterate: ___
Time to verify and fix output: ___
Total AI-assisted time: ___

Estimated time to do manually: ___

Was AI worth it? Why or why not?
```

## Effectiveness Checklist

| Question | Warning Sign |
|----------|--------------|
| Did I finish faster? | Spent more time prompting than doing |
| Is the output correct? | Had to fix major errors |
| Did I learn something? | Just copied without understanding |
| Would I do this again? | Relief when AI is unavailable |

## Task-Type Assessment

```
For [task type], evaluate AI assistance:

Success rate: __% of attempts produce usable output
Time savings: __% faster than manual
Quality: Better / Same / Worse than manual
Learning: Helped understand / Just generated output

Should I use AI for this task type? Yes / Sometimes / No
```

## When AI Helps vs. Hurts

### AI Usually Helps With:
- Boilerplate and ceremony
- Translation between formats
- Exploration and brainstorming
- Polish and style consistency

### AI Often Hurts With:
- Tasks you already do quickly
- Domains with high hallucination rate
- Critical decisions requiring deep expertise
- Work that takes longer to verify than create

## Tracking Template

Track AI usage for a week:

| Date | Task | AI Time | Fix Time | Manual Est. | Worth It? |
|------|------|---------|----------|-------------|-----------|
| | | | | | |
| | | | | | |

Patterns to look for:
- Task types where AI consistently helps
- Task types where AI consistently hurts
- Sweet spot for prompt complexity
